# Gridap_Test2.jl

# ----------------------------
# âœ… Load modules in dependency order
# ----------------------------
include("Data.jl")
include("LearnableVariables.jl")
include("Network.jl")
include("LossFunctional.jl")

using .Data
using .LearnableVariables
using .Network
using .LossFunctional
using LinearAlgebra
using Random

# ----------------------------
# âœ… Define main testing function
# ----------------------------
function main()
    println("=== GRIDAP MIGRATION TEST 1 ===")

    # ------------------------
    # ðŸ”§ PARAMETERS
    # ------------------------
    params = Dict(
        "fileName"         => "Resultados2.csv",
        "testRatio"        => 30.0,
        "polynomialOrder"  => 1,
        "xFeatures"        => [1, 2, 3, 4, 5, 6, 7],
        "yFeatures"        => [8],
        "HUtype"           => "ReLU",
        "OUtype"           => "linear",
        #"neuronsPerLayer"  => [5, 10, 1],   # Example: 5 inputs â†’ 10 hidden â†’ 1 output
        #"nLayers"          => 3,
        "hiddenLayers"     => [10],
        "costType"         => "L2"
    )

    # ------------------------
    # ðŸ§© INITIALIZE DATA
    # ------------------------
    data = Data.init_data(params)
    println("âœ” Data initialized.")
    println("   - Xtrain size: ", size(data.Xtrain))
    println("   - Ytrain size: ", size(data.Ytrain))
    params["data"] = data


    # ------------------------
    # ðŸ§© INITIALIZE NETWORK
    # ------------------------
    net = Network.init_network(params)
    println("âœ” Network initialized.")
    println("   - Layers: ", net.neurons_per_layer)
    params["network"] = net

    # ------------------------
    # ðŸ§© INITIALIZE LEARNABLE VARIABLES
    # ------------------------
    lv = Network.get_learnable_variables(params["network"])
    println("âœ” Learnable variables initialized.")
    println("   - Î¸ vector length: ", length(lv.thetavec))
    params["learnableVariables"] = lv

    
    # ------------------------
    # ðŸ§© INITIALIZE LOSS FUNCTIONAL
    # ------------------------
    lf = LossFunctional.init_lossfunctional(params)
    println("âœ” Loss functional initialized with cost type: ", lf.cost_type)

    # ------------------------
    # ðŸ”Ž TEST FORWARD + BACKWARD
    # ------------------------
    Î¸ = lv.thetavec
    j, grad = LossFunctional.compute_loss_and_gradient(lf, Î¸)
    println("ðŸ”§ Full-batch loss computed:")
    println("   - Loss value: ", j)
    println("   - Gradient norm: ", norm(grad))

    # ------------------------
    # ðŸ”Ž TEST MINI-BATCH COMPUTATION
    # ------------------------
    order = randperm(size(data.Xtrain, 1))
    i_batch = 1
    j_stoch, grad_stoch, is_depleted = LossFunctional.compute_stochastic_loss_and_gradient(lf, Î¸, i_batch, order, true)
    println("ðŸ”§ Mini-batch loss computed:")
    println("   - Loss value: ", j_stoch)
    println("   - Gradient norm: ", norm(grad_stoch))
    println("   - Dataset depleted: ", is_depleted)

    # ------------------------
    # ðŸ”Ž TEST TEST-ERROR (classification tasks)
    # ------------------------
    err = LossFunctional.get_test_error(lf, Î¸)
    println("âœ” Test error: ", err)

    println("=== TEST COMPLETED SUCCESSFULLY ===")
end

# ----------------------------
# âœ… Run main when script executed
# ----------------------------
main()
